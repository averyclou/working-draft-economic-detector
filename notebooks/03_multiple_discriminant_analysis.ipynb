{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Best Recession Predictors with MDA\n",
    "\n",
    "Time for the main event! We'll use Multiple Discriminant Analysis to figure out which economic indicators are the best at spotting recessions before they hit. Think of it as teaching our model to recognize the warning signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import notebook utilities\n",
    "from notebook_utils import init_notebook, load_data, display_data_info, save_figure\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize notebook environment\n",
    "init_notebook()\n",
    "\n",
    "# Import from econ_downturn package\n",
    "from econ_downturn import (\n",
    "    apply_mda, create_discriminant_time_series,\n",
    "    plot_feature_importance, plot_mda_projection, plot_discriminant_time_series\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Our Engineered Features\n",
    "\n",
    "First, we will load the datasets prepared during our feature engineering stage.\n",
    "\n",
    "To refresh your memory, here is a description of these datasets:\n",
    "\n",
    "1. Data with Features:\n",
    "\n",
    "    This is our basic transformed data, including all original indicators with the engineered lagged and smoothed variables.\n",
    "\n",
    "\n",
    "2. Normalized Data:\n",
    "\n",
    "    As discussed earlier, putting each of our macroeconomic indicators on the same numeric scale is key for our PCA and MDA steps. This data is ready to be fed into our PCA step.\n",
    "\n",
    "\n",
    "3. PCA Data:\n",
    "\n",
    "    This saved file includes only the principal components identified from PCA, as well as the target recession indicator. This is the most compact and analysis-ready version of our data, and is optimized for training our MDA model.\n",
    "\n",
    "\n",
    "These will become relevant as we get into model creation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data paths for loading processed data\n",
    "from econ_downturn import get_data_paths\n",
    "data_paths = get_data_paths()\n",
    "data_dir = data_paths['processed_dir']\n",
    "\n",
    "# Load the dataset with features\n",
    "features_path = os.path.join(data_dir, 'data_with_features.csv')\n",
    "if os.path.exists(features_path):\n",
    "    data_features = pd.read_csv(features_path, index_col=0, parse_dates=True)\n",
    "    print(f\"Loaded dataset with features, shape: {data_features.shape}\")\n",
    "else:\n",
    "    print(f\"Dataset with features not found at {features_path}\")\n",
    "    data_features = pd.DataFrame()\n",
    "\n",
    "# Load the normalized dataset\n",
    "normalized_path = os.path.join(data_dir, 'data_normalized.csv')\n",
    "if os.path.exists(normalized_path):\n",
    "    data_normalized = pd.read_csv(normalized_path, index_col=0, parse_dates=True)\n",
    "    print(f\"Loaded normalized dataset, shape: {data_normalized.shape}\")\n",
    "else:\n",
    "    print(f\"Normalized dataset not found at {normalized_path}\")\n",
    "    data_normalized = pd.DataFrame()\n",
    "\n",
    "# Load the PCA dataset\n",
    "pca_path = os.path.join(data_dir, 'data_pca.csv')\n",
    "if os.path.exists(pca_path):\n",
    "    data_pca = pd.read_csv(pca_path, index_col=0, parse_dates=True)\n",
    "    print(f\"Loaded PCA dataset, shape: {data_pca.shape}\")\n",
    "else:\n",
    "    print(f\"PCA dataset not found at {pca_path}\")\n",
    "    data_pca = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up for Analysis\n",
    "\n",
    "Before we start the model training, we need to implement the best practice of splitting data into training and testing subsets. The model should generalize the unseen data well to indicate success.\n",
    "\n",
    "We use a 70/30 split (70% for training, 30% for testing).\n",
    "\n",
    "First let's break down the prepare_data_for_mda() function that we established in our utility package. There are three base transformations and two outputs.\n",
    "\n",
    "#### Base Transformations:\n",
    "\n",
    "1. Validating inputs:\n",
    "    This tranformation checks whether the dataframe inputed in the function is empty or missing the recession flagging column. If so, the process is exited.\n",
    "\n",
    "2. Seperating the features and target indicator:\n",
    "    As with our previous work, we always need to seperate our features from our recession indicator. If our recession indicator was used as a feature, we would be using the target outcome within our MDA model. This would be redundant. Independent variables (our features) are extracted as X, while the dependent variable (the recession flag) is set to y.\n",
    "\n",
    "3. Training split:\n",
    "    As mentioned previously, we use a 70/30 training split. The test size is 30%, while the training size is 70%. This gives an optimal quantity of unseen data to test the final model on.\n",
    "\n",
    "note: Stratification is applied using y. This is to ensure that both the training and the test splits receive an equal proportion of recessionary periods.\n",
    "\n",
    "\n",
    "#### Function Outputs:\n",
    "\n",
    "1. The splits: The first tuple contains data (X_train, X_test, y_train, y_test) for modeling.\n",
    "\n",
    "2. The full data: (X, y) represents the full dataset (used later for projections and visualization).\n",
    "\n",
    "\n",
    "This function's described setup will be repeated independently for each of the three datasets:\n",
    "\n",
    "features: The original engineered variables.\n",
    "\n",
    "normalized: Scaled version the original.\n",
    "\n",
    "pca: Dimensionality-reduced version via PCA.\n",
    "\n",
    "The valid results are stored in the datasets dictionary. These are keyed by the dataset name. This enables us to easily loop through the dictionary and apply MDA consistently in the next section of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for MDA\n",
    "def prepare_data_for_mda(data):\n",
    "    if data.empty or 'recession' not in data.columns:\n",
    "        print(\"Data is empty or does not contain the recession indicator.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data.drop(columns=['recession'])\n",
    "    y = data['recession']\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test), (X, y)\n",
    "\n",
    "# Prepare the datasets for MDA\n",
    "datasets = {}\n",
    "\n",
    "if not data_features.empty and 'recession' in data_features.columns:\n",
    "    datasets['features'] = prepare_data_for_mda(data_features)\n",
    "    print(\"Prepared dataset with features for MDA\")\n",
    "\n",
    "if not data_normalized.empty and 'recession' in data_normalized.columns:\n",
    "    datasets['normalized'] = prepare_data_for_mda(data_normalized)\n",
    "    print(\"Prepared normalized dataset for MDA\")\n",
    "\n",
    "if not data_pca.empty and 'recession' in data_pca.columns:\n",
    "    datasets['pca'] = prepare_data_for_mda(data_pca)\n",
    "    print(\"Prepared PCA dataset for MDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the MDA Model\n",
    "\n",
    "This step will fit a Multiple Discriminant Analysis model (or MDA) to each prepared dataset to effectively classify recession periods. This is the key to our project.\n",
    "\n",
    "There are two core functions used to fit and evaluate the model:\n",
    "\n",
    "#### 1. apply_mda_and_evaluate()\n",
    "1a. This wrapper function handles the infastructure of the entire MDA pipeline for our dataset. It takes the train/test split and full dataset as inputs.\n",
    "\n",
    "2a. The full dataset (X_full, y_full) are passed to be used later for visualization.\n",
    "\n",
    "3a. The function returns four performance metrics after the model is fitted: accuracy, confusion matrix, classification report, and cross-validation scores.\n",
    "\n",
    "4a. The function displays and plots the top 15 predictors.\n",
    "\n",
    "#### 2. apply_mda()\n",
    "This function handles the actual training of the Linear Discriminant model\n",
    "This function, imported from the projectâ€™s modeling utilities, handles the actual training of the Linear Discriminant model.\n",
    "\n",
    "1b. The model is fit to the training data, and starts to generate predictions.\n",
    "\n",
    "2b. Also, it uses cross-validation on the training set to estimate generalizability.\n",
    "\n",
    "3b. Once the validation is finished, the model is refit on the entire dataset to prepare for visualization steps.\n",
    "\n",
    "4b. The final outputs will be a trained model, prediction scores, and a ranked list of importance of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply MDA and evaluate the model\n",
    "def apply_mda_and_evaluate(dataset_name, split_data, full_data):\n",
    "    if split_data is None or full_data is None:\n",
    "        print(f\"No data available for {dataset_name}\")\n",
    "        return None\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_data\n",
    "    X_full, y_full = full_data\n",
    "    \n",
    "    # Apply MDA using the package function (it handles train/test split internally)\n",
    "    mda_results = apply_mda(X_full, y_full, test_size=0.3, random_state=42)\n",
    "    \n",
    "    print(f\"\\nResults for {dataset_name} dataset:\")\n",
    "    print(f\"Accuracy: {mda_results['accuracy']:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(mda_results['conf_matrix'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(mda_results['class_report'])\n",
    "    print(f\"\\nCross-Validation Scores: {mda_results['cv_scores']}\")\n",
    "    print(f\"Mean CV Score: {mda_results['cv_scores'].mean():.4f}\")\n",
    "    \n",
    "    # Plot feature importances\n",
    "    if mda_results['feature_importance'] is not None:\n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        display(mda_results['feature_importance'].head(10))\n",
    "        \n",
    "        # Plot feature importances\n",
    "        fig = plot_feature_importance(mda_results['feature_importance'], top_n=15)\n",
    "        plt.title(f'Top 15 Feature Importances - {dataset_name} Dataset', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save the figure\n",
    "        save_figure(fig, f\"feature_importance_{dataset_name}.png\")\n",
    "    \n",
    "    return mda_results\n",
    "\n",
    "# Apply MDA to each dataset\n",
    "mda_results = {}\n",
    "\n",
    "for name, (split_data, full_data) in datasets.items():\n",
    "    mda_results[name] = apply_mda_and_evaluate(name, split_data, full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Our Results\n",
    "\n",
    "Two key plots will check how well our MDA model distinguishes between recessionary vs. non-recessionary periods.\n",
    "\n",
    "1. plot_mda_projection() shows a 2d projection of our model's decision space. This clearly shows how our model seperates the two binary classes. This gives a visual evaluation of the discriminative power of the model learned from training.\n",
    "\n",
    "2. create_discriminant_time_series() creates a time series of the discriminant function values using the trained MDA model. This is then visualized using plot_discriminant_time_series(). The goal of this is to highlight when the economy exhibits similar signals to historically recognized reccessions.\n",
    "\n",
    "Both plots are saved for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize MDA results\n",
    "def visualize_mda_results(dataset_name, mda_results, split_data, full_data):\n",
    "    if mda_results is None or split_data is None or full_data is None:\n",
    "        print(f\"No data available for {dataset_name}\")\n",
    "        return\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_data\n",
    "    X_full, y_full = full_data\n",
    "    \n",
    "    # Plot MDA projection (function extracts data from mda_results internally)\n",
    "    fig = plot_mda_projection(mda_results)\n",
    "    plt.title(f'MDA Projection - {dataset_name} Dataset', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    save_figure(fig, f\"mda_projection_{dataset_name}.png\")\n",
    "    \n",
    "    # Create discriminant time series\n",
    "    discriminant_df = create_discriminant_time_series(mda_results['model'], X_full, y_full)\n",
    "    \n",
    "    # Plot discriminant time series\n",
    "    fig = plot_discriminant_time_series(discriminant_df)\n",
    "    plt.title(f'Discriminant Function Over Time - {dataset_name} Dataset', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    save_figure(fig, f\"discriminant_time_series_{dataset_name}.png\")\n",
    "\n",
    "# Visualize MDA results for each dataset\n",
    "for name, results in mda_results.items():\n",
    "    if results is not None:\n",
    "        visualize_mda_results(name, results, datasets[name][0], datasets[name][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Indicators Matter Most?\n",
    "\n",
    "This step will contain model interpretation by examining the most influential features that were identified during MDA training.\n",
    "\n",
    "1. The identify_key_predictors() function will extract top features ranked by their importance (which was stored in the feature_importance output from MDA).\n",
    "\n",
    "2. It then will categorize these predictors into original indicators, lagged features, and rate-of-change metrics.\n",
    "\n",
    "3. This helps us better understand which features congtribute most significantly to recession detenction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify key recession predictors\n",
    "def identify_key_predictors(dataset_name, mda_results):\n",
    "    if mda_results is None or mda_results['feature_importance'] is None:\n",
    "        print(f\"No feature importance available for {dataset_name}\")\n",
    "        return\n",
    "    \n",
    "    feature_importance = mda_results['feature_importance']\n",
    "    \n",
    "    print(f\"\\nKey Recession Predictors from {dataset_name} Dataset:\")\n",
    "    \n",
    "    # Get the top 10 features\n",
    "    top_features = feature_importance.head(10)\n",
    "    display(top_features)\n",
    "    \n",
    "    # Categorize the features\n",
    "    categories = {\n",
    "        'Original': [],\n",
    "        'Lag': [],\n",
    "        'Rate of Change': []\n",
    "    }\n",
    "    \n",
    "    for feature in top_features['Feature']:\n",
    "        if '_lag' in feature:\n",
    "            categories['Lag'].append(feature)\n",
    "        elif '_pct_change' in feature or '_roc' in feature:\n",
    "            categories['Rate of Change'].append(feature)\n",
    "        else:\n",
    "            categories['Original'].append(feature)\n",
    "    \n",
    "    print(\"\\nFeatures by Category:\")\n",
    "    for category, features in categories.items():\n",
    "        print(f\"\\n{category} Features:\")\n",
    "        for feature in features:\n",
    "            print(f\"- {feature}\")\n",
    "\n",
    "# Identify key predictors for each dataset\n",
    "for name, results in mda_results.items():\n",
    "    if results is not None:\n",
    "        identify_key_predictors(name, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econ-downturn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
