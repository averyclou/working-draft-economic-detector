{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Better Features for Recession Prediction\n",
    "\n",
    "Now that we've explored our data, it's time to get it ready for machine learning. We'll clean up missing values, create some lag variables (because economic indicators often predict the future), and normalize everything so our model doesn't get confused by different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import notebook utilities\n",
    "from notebook_utils import (\n",
    "    # Setup functions\n",
    "    setup_notebook, load_data, display_data_info, save_figure,\n",
    "    \n",
    "    # Import from econ_downturn package\n",
    "    engineer_features, normalize_data, apply_pca\n",
    ")\n",
    "\n",
    "# Import other libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "# Set up the notebook environment\n",
    "setup_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Our Clean Dataset\n",
    "\n",
    "Let's grab the merged dataset we put together in our exploration phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data using the utility function\n",
    "merged_data = load_data(use_cached=True)\n",
    "\n",
    "# Display information about the dataset\n",
    "display_data_info(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Smart Features\n",
    "\n",
    "Time to engineer some features that will help our model spot recession patterns. We'll create lag variables, moving averages, and other transformations that capture how economic indicators behave over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer features using the package function\n",
    "data_with_features = engineer_features(merged_data)\n",
    "\n",
    "print(f\"Data shape after feature engineering: {data_with_features.shape}\")\n",
    "print(f\"Number of features: {data_with_features.shape[1]}\")\n",
    "\n",
    "# Display the first few rows of the engineered data\n",
    "display(data_with_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Everything on the Same Scale\n",
    "\n",
    "Different economic indicators have wildly different scales - unemployment might be 5% while GDP is in the trillions. Let's normalize everything so our model treats all features fairly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "data_normalized, scaler = normalize_data(data_with_features)\n",
    "\n",
    "print(f\"Data shape after normalization: {data_normalized.shape}\")\n",
    "\n",
    "# Display the first few rows of the normalized data\n",
    "display(data_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Complexity with PCA\n",
    "\n",
    "With all these features, we might have too much of a good thing. PCA will help us find the most important patterns in our data while reducing complexity and dealing with correlated variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = data_normalized.drop(columns=['recession'])\n",
    "y = data_normalized['recession']\n",
    "\n",
    "# Apply PCA\n",
    "X_pca_df, pca = apply_pca(X, n_components=0.95)\n",
    "\n",
    "# Calculate explained variance\n",
    "explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "print(f\"Data shape after PCA: {X_pca_df.shape}\")\n",
    "print(f\"Number of PCA components: {X_pca_df.shape[1] - 1}\")  # Subtract 1 for target column\n",
    "print(f\"Cumulative explained variance: {explained_variance:.4f}\")\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         np.cumsum(pca.explained_variance_ratio_), 'r-')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "save_figure(plt.gcf(), \"pca_explained_variance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Our Work\n",
    "\n",
    "Time to save all these processed datasets so we can use them in our modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data paths for saving processed data\n",
    "from econ_downturn import get_data_paths\n",
    "data_paths = get_data_paths()\n",
    "output_dir = data_paths['processed_dir']\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the dataset with features\n",
    "data_path = os.path.join(output_dir, 'data_with_features.csv')\n",
    "data_with_features.to_csv(data_path)\n",
    "print(f\"Saved dataset with features to {data_path}\")\n",
    "\n",
    "# Save the normalized dataset\n",
    "normalized_path = os.path.join(output_dir, 'data_normalized.csv')\n",
    "data_normalized.to_csv(normalized_path)\n",
    "print(f\"Saved normalized dataset to {normalized_path}\")\n",
    "\n",
    "# Save the PCA dataset\n",
    "pca_path = os.path.join(output_dir, 'data_pca.csv')\n",
    "X_pca_df.to_csv(pca_path)\n",
    "print(f\"Saved PCA dataset to {pca_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econ-downturn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
