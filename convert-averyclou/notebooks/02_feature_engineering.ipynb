{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Better Features for Recession Prediction\n",
    "\n",
    "Now that we've explored our data, it's time to get it ready for machine learning. We'll clean up missing values, create some lag variables (because economic indicators often predict the future), and normalize everything so our model doesn't get confused by different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import notebook utilities\n",
    "from notebook_utils import init_notebook, load_data, display_data_info, save_figure\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize notebook environment\n",
    "init_notebook()\n",
    "\n",
    "# Import from econ_downturn package\n",
    "from econ_downturn import engineer_features, normalize_data, apply_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Our Clean Dataset\n",
    "\n",
    "Let's grab the merged dataset we put together in our exploration phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data using the utility function\n",
    "merged_data = load_data(use_cached=True)\n",
    "\n",
    "# Display information about the dataset\n",
    "display_data_info(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Smart Features\n",
    "\n",
    "Our goal here is to transform raw indicators related to economic recession into informative inputs. engineer_features as a function will automate this process, generating lagging variables, moving averages, and other general transformations to capture indicator change over time.\n",
    "\n",
    "The engineered features displayed here are made to reflect the time-dependent aspect of economic signals. To use an example, a spike in unemployment could not indiciate a recession today, but could indicate one months later on. Through our inclusion of lagged indicator versions, our model is allowed to detect such delays. This results in a richer and more predictive dataset that reflects not only short-term shifts, but longer term trends that enhance our conclusions.\n",
    "\n",
    "The engineer_features function:\n",
    "\n",
    "1. Creates lagged versions of macroeconomic indicators.\n",
    "\n",
    "2. Calculates percentage changes over time to quantify shifts.\n",
    "\n",
    "3. Combines existing features (e.g. GPD & Unemployment) to measure joint effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer features using the package function\n",
    "data_with_features = engineer_features(merged_data)\n",
    "\n",
    "print(f\"Data shape after feature engineering: {data_with_features.shape}\")\n",
    "print(f\"Number of features: {data_with_features.shape[1]}\")\n",
    "\n",
    "# Display the first few rows of the engineered data\n",
    "display(data_with_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Everything on the Same Scale\n",
    "\n",
    "An initial limitation of the data we selected for analysis was the scale variance. These indicators have wildly different scales. For instance, unemployment rate might be 5% while GDP is in the trillions. Since these numeric metrics need to be fairly weighted and added together, it is crucial that we normalize the scale before building the MDA model. If we were to leave features unscaled, variables with larger numeric values would dominate the learning process.\n",
    "\n",
    "To avoid this, we will normalize the entire merged dataset using the normalize_data() function from our utility code. This rescales all our features to give them similar influence on an MDA model. Our normalized output preserves the original data structure while giving each variable equal influence.\n",
    "\n",
    "With our normalize_data function:\n",
    "\n",
    "1. A scaler is created to standardize the features.\n",
    "\n",
    "2. This scaler is fit onto our data and transforms all feature columns.\n",
    "\n",
    "3. The new DataFrame is saved in the data_normalized variable for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "data_normalized, scaler = normalize_data(data_with_features)\n",
    "\n",
    "print(f\"Data shape after normalization: {data_normalized.shape}\")\n",
    "\n",
    "# Display the first few rows of the normalized data\n",
    "display(data_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Complexity with PCA\n",
    "\n",
    "Next, we will use Principal Component Analysis (aka PCA) to reduce the dimensionality of our dataset. The ultimate goal is to simply the feature space listed while keeping the most important information. This step will help reduce any noise, redundancy, or multicollinearity reflected in the current merged dataset. We hope this feature simplification will improve the performance of our MDA model.\n",
    "\n",
    "There are a few important steps to note in this section:\n",
    "\n",
    "1. Seperating the target variable:\n",
    "\n",
    "    We removed the recession indicator column in order to only apply PCA to the predictor variable.\n",
    "\n",
    "\n",
    "2. Applying PCA:\n",
    "\n",
    "    Principal Component Analysis transforms our original features into a set of uncorrelated features that are called principal components. Each of these components is a combination of the original variables and captures a portion of overall dataset variance. We are essentially telling PCA to retain enough of these components to explain 95% of the whole variance, keeping the useful indicators while dropping less useful patterns.\n",
    "\n",
    "\n",
    "3. Understanding the Output:\n",
    "\n",
    "    We have created a new dataset with these principal components and added the recession indicator back into it for future modeling. This final dataset has less columns than the original merged dataset, but retains nearly all the important statistical patterns.\n",
    "    \n",
    "\n",
    "4. Our Plot:\n",
    "\n",
    "    The code outputs a bar chart and line graph to show how much variance is captured by each principal component. To explain the roles of each, the bar chart shows individual contributions of each comoponent, while the line shows the cumulative total. This is a visual way to verify that we retained all the critical information necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = data_normalized.drop(columns=['recession'])\n",
    "y = data_normalized['recession']\n",
    "\n",
    "# Apply PCA\n",
    "X_pca_df, pca = apply_pca(X, n_components=0.95)\n",
    "\n",
    "# Calculate explained variance\n",
    "explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "print(f\"Data shape after PCA: {X_pca_df.shape}\")\n",
    "print(f\"Number of PCA components: {X_pca_df.shape[1] - 1}\")  # Subtract 1 for target column\n",
    "print(f\"Cumulative explained variance: {explained_variance:.4f}\")\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         np.cumsum(pca.explained_variance_ratio_), 'r-')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "save_figure(plt.gcf(), \"pca_explained_variance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Our Work\n",
    "\n",
    "After our process of prepping data through feature engineering, normalization, and PCA, we save each version of the dataset to files. We can cleanly reuse these three new datasets in our modeling steps without repeating processing lines.\n",
    "\n",
    "Specifically, we save:\n",
    "\n",
    "1. Data with Features:\n",
    "\n",
    "    This is our basic transformed data, including all original indicators with the engineered lagged and smoothed variables.\n",
    "\n",
    "\n",
    "2. Normalized Data:\n",
    "\n",
    "    As discussed earlier, putting each of our macroeconomic indicators on the same numeric scale is key for our PCA and MDA steps. This data is ready to be fed into our PCA step.\n",
    "\n",
    "\n",
    "3. PCA Data:\n",
    "\n",
    "    This saved file will include only the principal components identified from PCA, as well as the target recession indicator. This is the most compact and analysis-ready version of our data, and is optimized for training our MDA model.\n",
    "\n",
    "\n",
    "Saving these for later use will cleanly consolidate our future steps by allowing us to easily import transformed data to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data paths for saving processed data\n",
    "from econ_downturn import get_data_paths\n",
    "data_paths = get_data_paths()\n",
    "output_dir = data_paths['processed_dir']\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the dataset with features\n",
    "data_path = os.path.join(output_dir, 'data_with_features.csv')\n",
    "data_with_features.to_csv(data_path)\n",
    "print(f\"Saved dataset with features to {data_path}\")\n",
    "\n",
    "# Save the normalized dataset\n",
    "normalized_path = os.path.join(output_dir, 'data_normalized.csv')\n",
    "data_normalized.to_csv(normalized_path)\n",
    "print(f\"Saved normalized dataset to {normalized_path}\")\n",
    "\n",
    "# Save the PCA dataset\n",
    "pca_path = os.path.join(output_dir, 'data_pca.csv')\n",
    "X_pca_df.to_csv(pca_path)\n",
    "print(f\"Saved PCA dataset to {pca_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econ-downturn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
